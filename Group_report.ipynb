{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis\n",
    "\n",
    "First we got to know the data by reading about the NPF-phenomena and browse trough previous research papers of this topic to get some understanding what we are going to classify. For the next step we got to know to the training data and created the *class2* variable on the basis of the *class4* variable. For the first binomial classification task we removed the *class4* since the predictable class was created base on it's values. We noticed that there are a lot of features and some of them were not appropiate features for classification task. We removed: *partlybad*, *index* and the *date* column. *Partly* bad had only *false* values so it was obvious that it will not give us any information related to the classification. The date column we did not delete completely but just changed it to be the indexing column or the training data.\n",
    "\n",
    "Now that the obvious features were removed the did statistical anaylysis to the training set. We created and correlation matrix to see how the features correlate with each other and also to the predictable class. Most of the correlation was between the measurments of the same substant on different heights. To get more detailed view of the full data we generated an report of the whole data set using *sweetviz* library. From the report we could easily see the distribution of each feature, min, max, median and lower and upper quartile information. From the report we could conclude that the distribution were not mostly Gaussian and that some of the features had also some outliers that needed to be dealt with. For the last thing in this part we also generated a profile for the data set. For this task we used *pandas_profiling* library. Here we could see that for each individual feature what are the other features that correlate whith it and could possibly give us some good information regarding to the classification task. *class2* variable correlated most with *RHIGA*, *RGLOB*, *GLOB* and *PAR* variables. This included the mean and std data on different heights of the *RHIGA* variable.\n",
    "\n",
    "After getting to know the data we were facing the feature selection task. We decided to use only the mean data since that possibly gives us more information than the standard deviation itself. This step reduced the features by nearly half. We assumed that the variables of measurements of same substance on different height are dependent of each other. These were also removed to have more reliable results on the classification task. We sticked with the measurements from the heighest part of the tower for each of the substance sensor. Since not all substances are collected for example on 72 meters we have substance measurements on different heights. But most importantly there is no substance feature multiple times in our training set. After the data wrangling we ended up having just 19 features from which 1 was the predictable class *class2*\n",
    "\n",
    "# Machine learning aproaches\n",
    "\n",
    "To select the best model for this task we decided to use atelast 1 discriminative model, 1 generative model, 1 probabilistic model and 1 not probabalistic model. We managed to use 2 probabilistic models which divided in the discriminative and generative model classes."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "43a1086d938abcc31e3623ae4ddc6301be053a38588b120e79a7921e5bfc801c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
